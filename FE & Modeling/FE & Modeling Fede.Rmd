---
title: "FE & Modeling"
output:
  html_document:
    df_print: paged
---

```{r setup, echo=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())

load_reqs <- function(reqs) {
  for(pkg in reqs) {
    if (!(pkg %in% installed.packages())) { install.packages(pkg) }
    suppressPackageStartupMessages(library(pkg, character.only = T))
  }
}

# ADD LIBRARIES HERE
load_reqs(c("tidyr", "dplyr", "data.table", "purrr", "ggplot2", "corrplot", 
            "gridExtra", "grid", "cowplot", "doParallel", "caret", "MLmetrics", "DMwR",
            "ROSE", "ranger", "xgboost", "deepboost"))
```

```{r echo=FALSE, warning=FALSE}
train <- read.csv('../data/BankCamp_train.csv')
submission <- read.csv('../data/BankCamp_test.csv')
```

### Visualization

#### Check if there are any missing values in the training set and the submission set

```{r echo=TRUE, warning=FALSE}
any(is.na(train))
any(is.na(submission))
```

**As both return FALSE, there are no missing values**

***

#### Check for the structure of the dataset
```{r echo=TRUE, warning=FALSE}
glimpse(train)
glimpse(submission)
```

**Comparing to training set with the submission set, we can tell the target variable is y**

***

#### Check for the factor levels of y
```{r echo=TRUE, warning=FALSE}
unique(train$y)
```

**As shown, it returns only yes and no, so it's a binary classification problem**

***

#### Check for imbalance
```{r echo=TRUE, warning=FALSE}
nrow(subset(train, y =='yes')) / nrow(train)
```

**11% ,  no over/under-sampling needed**

***

#### Numerical Features
```{r echo=FALSE, fig.height=6, warning=FALSE}

numeric_vars <- unlist(lapply(train, is.numeric)) 
histograms = list()
for (i in colnames(train[numeric_vars])){
  histograms[[i]] <-
    ggplot(
      train[numeric_vars],
      aes_string(x=i,fill=train$y)) +
      geom_histogram(bins = 35) +
      theme_bw(base_size = 13)+
      labs(fill = "Target")
}
plot_grid(plotlist = histograms, ncol = 2)
```

**As shown from the hists, most of the numerical features are heavily skewed, and need to be normalized**

***

#### Categorical Features
```{r echo=FALSE, fig.height=14, warning=FALSE}

cat_vars <- unlist(lapply(train, is.factor)) 
barPlots = list()
for (i in colnames(train[cat_vars])){
  if(i != 'y') {
      barPlots[[i]] <-
        ggplot(
          train[cat_vars],
          aes_string(x=i,fill=train$y)) +
          geom_bar() +
          theme_bw(base_size = 13)+
          labs(fill = "Target")
  }
}
plot_grid(plotlist = barPlots, ncol = 1)
```

**As shown from the hists, most of the numerical features are heavily skewed, and need to be normalized**

***

#### Correlation between two numerical features

```{r echo=TRUE, warning=FALSE}
train_num <- dplyr::select_if(train, is.numeric)
res <- cor(train_num)
corrplot.mixed(
          res,
          upper="circle",
          lower="number",
          tl.col = "black",
          number.cex = .8,
          tl.cex=.8)
```

**As shown from the result, there are no strong correlations between most of the pairs**

**Except the one for pdays and previous = 0.54**

#### numerical features with target
```{r}
numeric_vars <- unlist(lapply(train, is.numeric)) 
histograms = list()
for (i in colnames(train[numeric_vars])){
  histograms[[i]] <-
    ggplot(
      train[numeric_vars],
      aes_string(x=i,fill=train$y)) +
      geom_density(alpha = 0.25) +
      theme_minimal() +
      labs(fill = "Target")
}
plot_grid(plotlist = histograms, ncol = 2)

```


#### Modeling Preparation

```{r}
# encode target variable as 1 and 0 and convert it to factor
train$label <- as.factor(ifelse(train$y == 'yes', 1, 0))
train$y <- NULL

# split train data further into train and test (hold out)
test_proportion <- 0.2
set.seed(7)
train_index<-sample(nrow(train), floor(nrow(train)*(1-test_proportion)), replace = FALSE)

df_train <- train[train_index,]
df_test <- train[-train_index,]
```



### List of Possible Variables

Binning 
1. age
2. campaign
3. job = reduce the number of categories. take the low numbers and check the proportion of yes and no and merge only those which have a similar distribution.
4. poutcome = try binarizing it to success / no success


### Baseline - Binary Logistic
```{r}
# Define custom summary function so we can get recall as output always
# Include Accuracy as well so that it does not optimise for recall
# (and classify everything as positive)

recallSummary <- function (data,
    lev = NULL,
    model = NULL) {
    c(Accuracy = MLmetrics::Accuracy(data$pred, data$obs),
      recall = MLmetrics::Recall(data$obs, data$pred, positive = 1))
}

ctrl_base <- trainControl(
  method = "cv", # k-fold cross val
  number = 10, # k = 5
  savePredictions=TRUE,
  summaryFunction = recallSummary
)

log <- train(label~.,   # removing duration as it is highly associated with target
             data = df_train,
             method = 'glm',
             family = "binomial",
             trControl = ctrl_base)

log
```

The model performs poorly in terms of recall, probably due to the unbalanced target variable. This can be tackled by either undersampling the prevalent class or oversampling the rare class or both. 

### Baseline with Resampling
The target variable is highly unbalanced. We will therefore resample it in order try to make the model better capture the underrepresented class.

```{r}
# define basic train control object 
ctrl <- trainControl(method = "cv",
                     number = 10,
                     savePredictions=TRUE,
                     summaryFunction = recallSummary,
                     sampling = "down")
```

#### Undersampling
```{r}
set.seed(7)
log_under <- train(label~. ,
                   data = df_train,
                   method = 'glm',
                   family = "binomial",
                   trControl = ctrl)

log_under
```

This is a game changer. By simply undersampling, we are able to highly improve recall.

Let's try oversampling instead:
#### Oversampling
```{r}
ctrl$sampling <- "up"

set.seed(7)
log_up <- train(label~.,
                data = df_train,
                method = 'glm',
                family = "binomial",
                trControl = ctrl)

log_up
```

Using oversampling, the results are even better. Recall goes marginally down (by a few decimal points) and accuracy goes slightly up.

Next: "smote" and "rose" for sampling

#### SMOTE
```{r}
ctrl$sampling <- "smote"

set.seed(7)
log_smt <- train(label~.,
                 data = df_train,
                 method = 'glm',
                 family = "binomial",
                 trControl = ctrl)

log_smt
```

smote resampling takes accuracy up but brings down recall substantially. SMOTE stands for Synthetic Minority Oversampling Tecniques. It uses nearest neighbours algorithm to generate new and synthetic data. [1](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)

#### ROSE
```{r}
# ctrl$sampling <- "rose"
# 
# set.seed(7)
# log_rose <- train(label~.,
#                   data = df_train,
#                   method = 'glm',
#                   family = "binomial",
#                   trControl = ctrl)
# 
# log_rose
```

### Summary of all resampling results

```{r}
resample_models <- list(baseline = log,
                        undersampling = log_under,
                        oversampling = log_up,
                        SMOTE = log_smt
                        # ,ROSE = log_rose
                        )

# do a resampling on the models and generate distribution of accuracy and recall metrics
resampling <- resamples(resample_models)
summ1 <- summary(resampling)
summ1

# or just print the train results
summ2 <- NULL
for (i in 1:length(resample_models)) {
  res <- resample_models[[i]]$results[1:3]
  summ2 <- rbind(summ2, res)
}
summ2$parameter <- names(resample_models)
summ2
```


### Feature Preprocessing
Now that we have established that resampling improves model results significantly, we will keep it as the base and move to feature selection and feature engineering. We use oversampling method. 

#### Day as a categorical variable  
There is no numerical relevance to day of the month. Hence, it is converted as a factor. 

```{r}
train$day <- as.factor(train$day)
```


#### Age as a categorical variable
Proceed by grouping together levels of the job variable

```{r}
# create a new preprocessed dataset
train_preproc <- train

# bin age variable
train_preproc$age_bin <- cut(train_preproc$age, breaks = c(-Inf, 20, 40, 60, Inf),
                             labels = c('<20', "20-40", "40-60", ">60"))
train_preproc$age <- NULL
```

```{r}
df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
ctrl$sampling <- "up"

set.seed(7)
log_agebin <- train(label~.,
                    data = df_train_preproc,
                    method = 'glm',
                    family = "binomial",
                    trControl = ctrl)

log_agebin
```

```{r}
# update pipeline
summ2 <- rbind(summ2, log_agebin$results[1:3])
summ2$parameter[nrow(summ2)] <- "agebin"
summ2
```


The results after binning age improve slightly. We now move on to the next variable, jobs. 

#### Reducing the number of categories in job

```{r}
levels(df_train_preproc$job)
```

Job currently has 12 different levels, but the proportion of observations in each of them is not uniform. We try merging some levels together to see if it has any impact on the model performance. We realise that reducing the levels will result in loss of granularity but it is assumed that strategies are not going to be planned at such minute levels and a general idea of customer's occupation would be enough.    
```{r}
train_preproc$job_binned <- car::recode(train_preproc$job, "c('retired', 'student') = 'rs';
                                        c('blue-collar', 'entrepreneur')= 'be';
                                        c('housemaid', 'services')= 'hs'; 
                                        c('admin.', 'unknown')= 'au';
                        c('management', 'self-employed','technician', 'unemployed') = 'others'")
train_preproc$job_binned <- factor(train_preproc$job_binned)

train_preproc$job <- NULL
```

```{r}
df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
ctrl$sampling <- "up"

set.seed(7)
log_agejobbin <- train(label~.,
                    data = df_train_preproc,
                    method = 'glm',
                    family = "binomial",
                    trControl = ctrl)

log_agejobbin
```

```{r}
# update pipeline
summ2 <- rbind(summ2, log_agejobbin$results[1:3])
summ2$parameter[nrow(summ2)] <- "agejobbin"
summ2
```

The model recall decreases marginally. However, we can still go ahead with these two new features. 



#### Converting 'balance' to categorical 
This variable is highly skewed with negative values and majority 0. There are 3052 negative values and 2799 0s. There are two ways of addressing this problem. Either transform the variable using YeoJohnsson method which takes into account both negative and positive values unlike the Box-Cox method or to convert them to categorical variables. We start with the second option. 

```{r}
train_preproc$balance_bin <- cut(train_preproc$balance, 
                                 breaks = c(-Inf, 0, 500, 1000, 5000, Inf),
                                 labels = c("<=0", "1-500", "501-1000", "1001-5000", ">5000"))


train_preproc$balance <- NULL
```

```{r}
df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
ctrl$sampling <- "up"

set.seed(7)
log_bal <- train(label~.,
                 data = df_train_preproc,
                 method = 'glm',
                 family = "binomial",
                 trControl = ctrl)

log_bal
```

```{r}
# update pipeline
summ2 <- rbind(summ2, log_bal$results[1:3])
summ2$parameter[nrow(summ2)] <- "bal"
summ2
```

Binning balance improves accuracy and recall.  

#### Modify Day Variable
```{r}
#Some variables such as day (the day of the month) have a very high number of levels which hold little meaning.

ggplot(train_preproc, aes(x = day, fill = label)) +
  geom_bar(stat = "count", position = "fill")
```

```{r}
train_preproc$day_binned <- car::recode(train_preproc$day, "c(1,10,30) = 'special';
                        c(2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,19,20,21,
                        22,23,24,25,26,27,28,29,31) = 'others'")
train_preproc$day_binned <- factor(train_preproc$day_binned)

train_preproc$day <- NULL

df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
log_day_binned <- train(label~.,
                 data = df_train_preproc,
                 method = 'glm',
                 family = "binomial",
                 trControl = ctrl)

log_day_binned
```

```{r}
summ2 <- rbind(summ2, log_day_binned$results[1:3])
summ2$parameter[nrow(summ2)] <- "Day binned"
summ2
```


#### Redesigning how numeric variables are modelled

We now fit the model again, however, adding in YeoJohnsson through preprocessing to fix skewness in all numeric variables. No other preprocessing is done. 

```{r}
ctrl$sampling <- "up"

log_yj <- train(label~.,
                data = df_train,
                method = 'glm',
                family = "binomial",
                preProcess = "YeoJohnson",
                trControl = ctrl)

log_yj
```

The results remain mostly unchanged.
```{r}
# update pipeline
summ2 <- rbind(summ2, log_yj$results[1:3])
summ2$parameter[nrow(summ2)] <- "Yeo-Johnson"
summ2
```
The Yeo-Johnson transformation does not appear to help with fixing skewness and outlier issues in certain variables.

Let's try taking the logarithm instead
```{r}
# Logarithmise skewed variables
to_log <- function(df, l_varlist){
  for(l_var in l_varlist){
    df[,l_var] <- log(df[,l_var])
  }
  return(df)
}

# Select the numeric variables that are skewed
log_varlist <- list('duration', 'campaign', 'pdays', 'previous')

train_log <- train_preproc

# Add constant in order to avoid non-positive values in log (monotonic transformation)
train_log$duration <- train_log$duration + 1
train_log$pdays <- train_log$pdays + 2
train_log$previous <- train_log$previous + 1

train_log <- to_log(train_log, log_varlist)

df_train_preproc <- train_log[train_index,]
df_test_preproc <- train_log[-train_index,]
```

```{r}
ctrl$sampling <- "up"

log_log <- train(label~.,
                data = df_train_preproc,
                method = 'glm',
                family = "binomial",
                trControl = ctrl)

print(log_log)

summ2 <- rbind(summ2, log_log$results[1:3])
summ2$parameter[nrow(summ2)] <- "Logarithm"
summ2
```

#### Parallelize
```{r}
cl <- makePSOCKcluster(4) # I have 4 cores, this can be adapted
registerDoParallel(cl)
# All subsequent models are run in parallel
```

## Optimization
Different models & hyperparameter tuning

```{r}
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions=TRUE,
                     summaryFunction = recallSummary,
                     sampling = "up")
ctrl$sampling <- "up"
set.seed(7)
tuneGrid_ranger <- data.table(expand.grid(mtry=c(round(sqrt(length(df_train_preproc)) + 1)
                                                 ), #standard choices for mtry
                              splitrule= 'gini',
                              min.node.size=c(500, 700, 1000)))

ranger <- train(label~.,
                data = df_train_preproc,
                method = "ranger", num.trees=100,
                tuneGrid = tuneGrid_ranger,
                metric = "recall",
                trControl = ctrl)

ranger

#summ2 <- rbind(summ2, ranger$results[1:3])
#summ2$parameter[nrow(summ2)] <- "RF"
#summ2
```

```{r}
mypred_ranger <- predict(ranger, newdata = df_test_preproc)
Accuracy(mypred_ranger, df_test_preproc$label)
Recall(df_test_preproc$label, mypred_ranger, positive = 1)
```


Gradient Boosted Trees
100, 3, e:   0.8447847  0.8411017
200, 3, e:   0.8491741  0.8324347
100, 5, e:   0.8614778  0.7801673
100, 3, e, lambda 0.1: 0.8438514  0.8524467
100, 3, e, l:0.1, b: 0.1: 0.8293706  0.8539473
100, 3, e, l:0.13, b: 0.1:0.8284367  0.8503603

```{r}
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions=TRUE,
                     summaryFunction = recallSummary,
                     sampling = "up")

tunegrid_deepbost <- expand.grid(num_iter = c(100),
                                 tree_depth = c(3),
                                 beta = c(0.1),
                                 lambda = 0.1,
                                 loss_type = "e")

boosting <- train(label~.,
                data = df_train_preproc,
                method = "deepboost",
                tuneGrid = tunegrid_deepbost,
                trControl = ctrl,
                verbose = FALSE)

boosting
```

Try:
svmLinearWeights2 (loss!)
'svmLinear3'
regLogistic


XGBoost
```{r}
set.seed(7)

ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions=TRUE,
                     summaryFunction = recallSummary,
                     sampling = "up")

tunegrid_xgb <- expand.grid(nrounds = c(100, 50),
                       max_depth = c(5),
                       colsample_bytree = c(0.7),
                       eta = c(0.05, 0.03),
                       gamma = c(0.1, 0.2),
                       min_child_weight = c(1),
                       subsample = 1
                      )

xgb <- train(label~.,
                data = df_train_preproc,
                method = "xgbTree",
                tuneGrid = tunegrid_xgb,
             metric = "recall",
                trControl = ctrl)

xgb
```



```{r}
mypred_xgb <- predict(xgb, newdata = df_test_preproc)
Accuracy(mypred_xgb, df_test_preproc$label)
Recall(df_test_preproc$label, mypred_xgb, positive = 1)
```


Logistic Regression with regularization
```{r}
ctrl$sampling <- "up"

tuneGrid_reg = expand.grid(alpha = c(0, 0.5, 1),
                                       lambda = c(0, 0.2, 0.5)) #alpha = 0 for Ridge and alpha = 1 for lasso

log_reg <- train(label~.,
                data = df_train_preproc,
                method = 'glmnet',
                family = "binomial",
                trControl = ctrl,
                tuneGrid = tuneGrid_reg)

print(log_reg)

summ2 <- rbind(summ2, log_log$results[1:3])
summ2$parameter[nrow(summ2)] <- "Logistic Regularized"
summ2
```
Regularization significantly worsens performance.