---
title: "FE & Modeling"
output:
  html_document:
    df_print: paged
---

```{r setup, echo=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())

# Install and load necessary libraries
load_reqs <- function(reqs) {
  for(pkg in reqs) {
    if (!(pkg %in% installed.packages())) { install.packages(pkg) }
    suppressPackageStartupMessages(library(pkg, character.only = T))
  }
}

# ADD LIBRARIES HERE
load_reqs(c("tidyr", "dplyr", "data.table", "purrr", "ggplot2", "corrplot", 
            "gridExtra", "grid", "cowplot", "doParallel", "caret", "MLmetrics", "DMwR",
            "ROSE", "ranger", "xgboost", "deepboost"))
```

```{r echo=FALSE, warning=FALSE}
# Load the data
train <- read.csv('../data/BankCamp_train.csv')
submission <- read.csv('../data/BankCamp_test.csv')
```

### Visualization

#### Check if there are any missing values in the training set and the submission set

```{r echo=TRUE, warning=FALSE}
any(is.na(train))
any(is.na(submission))
```

**As both return FALSE, there are no missing values**

***

#### Check for the structure of the dataset
```{r echo=TRUE, warning=FALSE}
glimpse(train)
glimpse(submission)
```

**Comparing to training set with the submission set, we can tell the target variable is y**

***

#### Check for the factor levels of y
```{r echo=TRUE, warning=FALSE}
unique(train$y)
```

**As shown, it returns only yes and no, so it's a binary classification problem**

***

#### Check for imbalance
```{r echo=TRUE, warning=FALSE}
nrow(subset(train, y =='yes')) / nrow(train)
```

**11%, resampling may be necessary to capture the target well**

***

#### Numerical Features
```{r echo=FALSE, fig.height=6, warning=FALSE}

numeric_vars <- unlist(lapply(train, is.numeric)) 
histograms = list()
for (i in colnames(train[numeric_vars])){
  histograms[[i]] <-
    ggplot(
      train[numeric_vars],
      aes_string(x=i,fill=train$y)) +
      geom_histogram(bins = 35) +
      theme_bw(base_size = 13)+
      labs(fill = "Target")
}
plot_grid(plotlist = histograms, ncol = 2)
```

**As can be seen from the histograms, most of the numerical features are heavily skewed, and need to be normalized**

***

#### Categorical Features
```{r echo=FALSE, fig.height=14, warning=FALSE}

cat_vars <- unlist(lapply(train, is.factor)) 
barPlots = list()
for (i in colnames(train[cat_vars])){
  if(i != 'y') {
      barPlots[[i]] <-
        ggplot(
          train[cat_vars],
          aes_string(x=i,fill=train$y)) +
          geom_bar() +
          theme_bw(base_size = 13)+
          labs(fill = "Target")
  }
}
plot_grid(plotlist = barPlots, ncol = 1)
```

***

#### Correlation between pairs of numerical features

```{r echo=TRUE, warning=FALSE}
train_num <- dplyr::select_if(train, is.numeric)
res <- cor(train_num)
corrplot.mixed(
          res,
          upper="circle",
          lower="number",
          tl.col = "black",
          number.cex = .8,
          tl.cex=.8)
```

**As shown from the result, there are no strong correlations between most of the pairs**

**Except the one for pdays and previous = 0.54**

***

#### Numerical features with target
```{r}
numeric_vars <- unlist(lapply(train, is.numeric)) 
histograms = list()
for (i in colnames(train[numeric_vars])){
  histograms[[i]] <-
    ggplot(
      train[numeric_vars],
      aes_string(x=i,fill=train$y)) +
      geom_density(alpha = 0.25) +
      theme_minimal() +
      labs(fill = "Target")
}
plot_grid(plotlist = histograms, ncol = 2)

```

**Apart from duration, none of the numerical variables clearly separates the target classes**

***

#### Preprocessing

```{r}
# encode target variable as 1 and 0 and convert it to factor
train$label <- as.factor(ifelse(train$y == 'yes', 1, 0))
train$y <- NULL

# split train data further into train and test (hold out)
test_proportion <- 0.2
set.seed(7)
train_index<-sample(nrow(train), floor(nrow(train)*(1-test_proportion)), replace = FALSE)

df_train <- train[train_index,]
df_test <- train[-train_index,]
```


## Baseline - Binary Logistic
```{r}
# Define custom summary function so we can get recall as output always

recallSummary <- function (data,
    lev = NULL,
    model = NULL) {
    c(Accuracy = MLmetrics::Accuracy(data$pred, data$obs),
      recall = MLmetrics::Recall(data$obs, data$pred, positive = 1))
}

ctrl_base <- trainControl(
  method = "cv", # k-fold cross val
  number = 5,
  savePredictions=TRUE,
  summaryFunction = recallSummary
)

bsline <- train(label~.,
             data = df_train,
             method = 'glm',
             family = "binomial",
             trControl = ctrl_base)

bsline
```

The model performs poorly in terms of recall, probably due to the unbalanced target variable. This can be tackled by either undersampling the prevalent class or oversampling the rare class or both. 

### Baseline with Resampling
The target variable is highly unbalanced. We will therefore resample it in order try to make the model better capture the underrepresented class.

Resampling is defined through the control object instead of outside, such that cross-validation performance will be evaluated on data that has not been resampled. Failing to do so would artifially inflate cv-recall, with a model that would perform far worse on an unbalanced test set.

```{r}
# define basic train control object 
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions=TRUE,
                     summaryFunction = recallSummary,
                     sampling = "down")
```

#### Undersampling
```{r}
set.seed(7)
log_under <- train(label~. ,
                   data = df_train,
                   method = 'glm',
                   family = "binomial",
                   trControl = ctrl)

log_under
```

This is a game changer. By simply undersampling, we are able to highly improve recall.

Let's try oversampling instead:
#### Oversampling
```{r}
ctrl$sampling <- "up"

set.seed(7)
log_up <- train(label~.,
                data = df_train,
                method = 'glm',
                family = "binomial",
                trControl = ctrl)

log_up
```

Using oversampling, the results are even better. Both recall and accuracy improve marginally.

Next: "smote" and "rose" for sampling

#### SMOTE
```{r}
ctrl$sampling <- "smote"

set.seed(7)
log_smt <- train(label~.,
                 data = df_train,
                 method = 'glm',
                 family = "binomial",
                 trControl = ctrl)

log_smt
```

Smote resampling takes accuracy up but brings down recall substantially. SMOTE stands for Synthetic Minority Oversampling Tecniques. It uses nearest neighbours algorithm to generate new and synthetic data. [1](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)

### Summary of all resampling results

```{r}
resample_models <- list(baseline = bsline,
                        undersampling = log_under,
                        oversampling = log_up,
                        SMOTE = log_smt
                        )

# do a resampling on the models and generate distribution of accuracy and recall metrics
resampling <- resamples(resample_models)
summ1 <- summary(resampling)
summ1

# or just print the train results
summ2 <- NULL
for (i in 1:length(resample_models)) {
  res <- resample_models[[i]]$results[1:3]
  summ2 <- rbind(summ2, res)
}
summ2$parameter <- names(resample_models)
summ2
```


### Feature Engineering
Now that we have established that resampling improves model results significantly, we will keep it as the base and move to feature selection and feature engineering. We use oversampling method. 

#### Day as a categorical variable  
There is no numerical relevance to day of the month. Hence, it is converted as a factor. 

```{r}
train$day <- as.factor(train$day)
```


#### Age as a categorical variable
Proceed by grouping together age-ranges

```{r}
# create a new preprocessed dataset
train_preproc <- train

# bin age variable
train_preproc$age_bin <- cut(train_preproc$age, breaks = c(-Inf, 20, 40, 60, Inf),
                             labels = c('<20', "20-40", "40-60", ">60"))
train_preproc$age <- NULL
```

```{r}
df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
ctrl$sampling <- "up"

set.seed(7)
log_agebin <- train(label~.,
                    data = df_train_preproc,
                    method = 'glm',
                    family = "binomial",
                    trControl = ctrl)

log_agebin
```

```{r}
# update pipeline
summ2 <- rbind(summ2, log_agebin$results[1:3])
summ2$parameter[nrow(summ2)] <- "age binned"
summ2
```

The results after binning age improve slightly. We now move on to the next variable, jobs. 

#### Reducing the number of categories in job

```{r}
levels(df_train_preproc$job)
```

Job currently has 12 different levels, but the proportion of observations in each of them is not uniform. We try merging some levels together to see if it has any impact on the model performance. We realise that reducing the levels will result in loss of granularity but it is assumed that strategies are not going to be planned at such minute levels and a general idea of customer's occupation would be enough.    
```{r}
train_preproc$job_binned <- car::recode(train_preproc$job, "c('retired', 'student') = 'rs';
                                        c('blue-collar', 'entrepreneur')= 'be';
                                        c('housemaid', 'services')= 'hs'; 
                                        c('admin.', 'unknown')= 'au';
                        c('management', 'self-employed','technician', 'unemployed') = 'others'")
train_preproc$job_binned <- factor(train_preproc$job_binned)

train_preproc$job <- NULL
```

```{r}
df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
set.seed(7)
log_agejobbin <- train(label~.,
                    data = df_train_preproc,
                    method = 'glm',
                    family = "binomial",
                    trControl = ctrl)

log_agejobbin
```

```{r}
# update pipeline
summ2 <- rbind(summ2, log_agejobbin$results[1:3])
summ2$parameter[nrow(summ2)] <- "age & job binned"
summ2
```

The model recall decreases marginally. However, we can still go ahead with these two new features. 

#### Converting 'balance' to categorical 
This variable is highly skewed with negative values and majority 0. There are 3052 negative values and 2799 0s. We group the variable into ranges in order to cope with this.

```{r}
train_preproc$balance_bin <- cut(train_preproc$balance, 
                                 breaks = c(-Inf, 0, 500, 1000, 5000, Inf),
                                 labels = c("<=0", "1-500", "501-1000", "1001-5000", ">5000"))


train_preproc$balance <- NULL
```

```{r}
df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
set.seed(7)
log_bal <- train(label~.,
                 data = df_train_preproc,
                 method = 'glm',
                 family = "binomial",
                 trControl = ctrl)

log_bal
```

```{r}
# update pipeline
summ2 <- rbind(summ2, log_bal$results[1:3])
summ2$parameter[nrow(summ2)] <- "balance binned"
summ2
```

Binning balance improves accuracy and recall.  

#### Modify Day Variable
The day variable is categorical not, but that means that it has 31 different levels. It can be imagined that certain days of the month have a slight effect on the target but most do not hold any explanatory power.

```{r}
ggplot(train_preproc, aes(x = day, fill = label)) +
  geom_bar(stat = "count", position = "fill")
```

It is reasonable to argue that the first and the last day of the month are somewhat special. We group them together and put the rest into one category.

```{r}
train_preproc$day_binned <- car::recode(train_preproc$day, "c(1,30) = 'special';
                        c(2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,
                        22,23,24,25,26,27,28,29,31) = 'others'")
train_preproc$day_binned <- factor(train_preproc$day_binned)

train_preproc$day <- NULL

df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
log_day_binned <- train(label~.,
                 data = df_train_preproc,
                 method = 'glm',
                 family = "binomial",
                 trControl = ctrl)

log_day_binned
```

```{r}
summ2 <- rbind(summ2, log_day_binned$results[1:3])
summ2$parameter[nrow(summ2)] <- "Day binned"
summ2
```

Model performance goes down but we are significantly reducing the number of variables which will shorten runtimes.

#### Redesigning how numeric variables are modelled

We now fit the model again, however, adding in Yeo-Johnson through preprocessing to fix skewness in all numeric variables. No other preprocessing is done. 

The Yeo-Johnson transformation is a variant of the BoxCox Transformation suited for variables with non-positive values. Variables are transformed to a more normal-like distribution with more stable variance.

```{r}
ctrl$sampling <- "up"

log_yj <- train(label~.,
                data = df_train,
                method = 'glm',
                family = "binomial",
                preProcess = "YeoJohnson",
                trControl = ctrl)

log_yj
```

The results remain mostly unchanged.
```{r}
# update pipeline
summ2 <- rbind(summ2, log_yj$results[1:3])
summ2$parameter[nrow(summ2)] <- "Yeo-Johnson"
summ2
```

The Yeo-Johnson transformation does not appear to help with fixing skewness and outlier issues in certain variables.

#### Logarithm
Let's try taking the logarithm instead
```{r}
# Logarithmise skewed variables
to_log <- function(df, l_varlist){
  for(l_var in l_varlist){
    df[,l_var] <- log(df[,l_var])
  }
  return(df)
}

# Select the numeric variables that are skewed
log_varlist <- list('duration', 'campaign', 'pdays', 'previous')

train_log <- train_preproc

# Add constant in order to avoid non-positive values in log (monotonic transformation)
train_log$duration <- train_log$duration + 1
train_log$pdays <- train_log$pdays + 2
train_log$previous <- train_log$previous + 1
#train_log$balance <- train_log$balance + 10000

train_log <- to_log(train_log, log_varlist)

df_train_preproc <- train_log[train_index,]
df_test_preproc <- train_log[-train_index,]
```

```{r}
logarithm <- train(label~.,
                data = df_train_preproc,
                method = 'glm',
                family = "binomial",
                trControl = ctrl)

print(logarithm)

summ2 <- rbind(summ2, logarithm$results[1:3])
summ2$parameter[nrow(summ2)] <- "Logarithm"
summ2
```

#### Parallelize
```{r}
cl <- makePSOCKcluster(4) # I have 4 cores, this can be adapted
registerDoParallel(cl)
# All subsequent models are run in parallel
```

## Optimization
Different models & hyperparameter tuning

### Random Forest
```{r}
set.seed(7)
# Several grids were explored, adapting the parameters in the direction where performance was improving.
# For faster runtimes, only a small grid is ran.
tuneGrid_ranger <- data.table(expand.grid(mtry=c(round(sqrt(length(df_train_preproc)) + 1)
                                                 #round(1/2 * (length(df_train_preproc)))
                                                 ), #standard choices for mtry
                              splitrule= 'gini',
                              min.node.size=500))
# We found that very large values for min.node.size lead to the highest recall. Lower values optimize
# accuracy at the cost of recall.

ranger <- train(label~.,
                data = df_train_preproc,
                method = "ranger", num.trees=100, # increasing the number of trees, the model did not improve further
                tuneGrid = tuneGrid_ranger,
                metric = "recall",
                importance = "permutation",
                trControl = ctrl)

ranger

#summ2 <- rbind(summ2, ranger$results[1:3])
#summ2$parameter[nrow(summ2)] <- "RF"
#summ2
```

Calculate test set performance
```{r}
pred_ranger <- predict(ranger, newdata = df_test_preproc)
Accuracy(pred_ranger, df_test_preproc$label)
Recall(df_test_preproc$label, pred_ranger, positive = 1)
```


### Boosting trees

Deepboost is a crazy variant of boosting. It is optimized to be able to grow ensembles of very deep (very large) trees without overfitting like AdaBoost will when growing very deep trees. We grow very shallow trees with it and only select it because it allows to choose between logistic and exponential for the loss function. Logistic loss never assigns zero penalty to any points, making it more sensitive to outliers. Exponential loss penalizes incorrect corrections more strongly.

```{r}
# Again here, a far more exhaustive grid was explored iteratively.
tunegrid_deepbost <- expand.grid(num_iter = c(100), #more and larger trees improved accuracy at the cost of recall
                                 tree_depth = c(3),
                                 beta = c(0.1),
                                 lambda = 0.1,
                                 loss_type = c("e", "l"))

boosting <- train(label~.,
                data = df_train_preproc,
                method = "deepboost",
                tuneGrid = tunegrid_deepbost,
                trControl = ctrl,
                verbose = FALSE)

boosting
```

Deepboost is not able to reach the levels of recall obtained with the random forest.


### XGBoost

```{r}
set.seed(7)

tunegrid_xgb <- expand.grid(nrounds = c(100, 80),
                       max_depth = c(5),
                       colsample_bytree = c(0.7),
                       eta = c(0.05),
                       gamma = c(0.1, 0.2),
                       min_child_weight = c(1),
                       subsample = 1
                      )

xgb <- train(label~.,
                data = df_train_preproc,
                method = "xgbTree",
                tuneGrid = tunegrid_xgb,
             metric = "recall",
                trControl = ctrl)

xgb
```

XGBoost performs quite well but still worse than random forest.

```{r}
mypred_xgb <- predict(xgb, newdata = df_test_preproc)
Accuracy(mypred_xgb, df_test_preproc$label)
Recall(df_test_preproc$label, mypred_xgb, positive = 1)
```


### Logistic Regression with regularization
Since we have seen that recall seems to be highest the less granularly the model fits the data, we will try whether a regularized logistic regression performs well.

```{r}
tuneGrid_reg = expand.grid(alpha = c(0, 0.5, 1), #alpha = 0 for Ridge and alpha = 1 for lasso
                                       lambda = c(0, 0.2, 0.5)) # higher lambda: more regularization

log_reg <- train(label~.,
                data = df_train_preproc,
                method = 'glmnet',
                family = "binomial",
                trControl = ctrl,
                tuneGrid = tuneGrid_reg)

print(log_reg)

#summ2 <- rbind(summ2, log_log$results[1:3])
#summ2$parameter[nrow(summ2)] <- "Logistic Regularized"
#summ2
```
Regularization significantly worsens performance.


### XGBoost optimized for lift

One last iteration was tried. Unfortunatly, caret does not allow for modification of the loss function. Ideally, we would want a cost function that highly penalizes false negatives, more so than false positives.

XGBoost allows for a custom evaluation function that goes a bit further than the custom evaluation function in caret. While we can not modify the loss function, the evaluation metric can be set 

```{r}
xgb_params <- list(objective = "binary:logistic",
                   eta = 0.05,
                   max_depth = 15,
                   colsample_bytree = 0.7,
                   subsample = 1,
                   min_child_weight = 1)

getProfit <- function(obs, pred, threshold = 0.5) {
  prob <- pred
  pred <- ifelse(pred >= threshold, 1, 0)
  tp_count <- sum(obs == 1 & obs == pred)
  tn_count <- sum(obs == 0 & obs == pred)
  fp_count <- sum(obs == 0 & obs != pred)
  fn_count <- sum(obs == 1 & obs != pred)
  profit <- tp_count * 3 - fn_count * 10
  #profit <- 10*tp_count - 3 * fp_count
  
  return(profit)
}

getBenchmarkProfit <- function(obs) { # predict non-returning for everyone
  n <- length(obs)
  getProfit(obs, rep(0, times = n))
}

getLift <- function(probs, labels) {
  pred_profit <- as.numeric(getProfit(obs = labels,
                                      pred = probs,
                                      threshold = 0.5))
  naive_profit <- as.numeric(getBenchmarkProfit(labels))
  profit_lift <- pred_profit/naive_profit
  return(profit_lift)
}

xgb.getLift <- function(preds, data) {
    labels <- getinfo(data, "label")
    lift <- getLift(preds, labels)
    return(list(metric = "Lift", value = lift))
}

df_train_resampled <- upSample(x = df_train_preproc[, -13],
                     y = df_train_preproc$label)    
df_test_notresampled <- df_test_preproc
df_test_notresampled$Class <- df_test_notresampled$label
df_test_notresampled$label <- NULL

datamatrix <- model.matrix(Class~., data = df_train_resampled)[, -1]
y <- as.numeric(df_train_resampled$Class) - 1
testdatamatrix <- model.matrix(Class~., data = df_test_notresampled)[, -1]
ytest <- as.numeric(df_test_notresampled$Class) - 1

xgb_fit <- xgb.cv(params = xgb_params, data = datamatrix, label = y, nfold = 5,
                  feval = xgb.getLift, maximize = TRUE, 
                  nrounds = 500, early_stopping_rounds = 50,
                  verbose = TRUE, print_every_n = 100)
```


```{r}
xgb_pred<- xgb.train(params = xgb_params, data = xgb.DMatrix(data = datamatrix, label = y),
                  feval = xgb.getLift, maximize = TRUE, 
                  nrounds = 500, early_stopping_rounds = 50,
                  watchlist = list(val = xgb.DMatrix(data = testdatamatrix, label = ytest)),
                  verbose = TRUE, print_every_n = 100)
```

```{r}
temp_prob <- predict(xgb_pred, newdata = xgb.DMatrix(data = testdatamatrix, label = ytest), 
                      type = 'response')
temp_preds <- ifelse(temp_prob >= 0.5, 1, 0)
Accuracy(temp_preds, ytest)
Recall(ytest, temp_preds, positive = 1)
```


## Final Prediction

```{r}
# Prepare submission dataset

submission$day <- as.factor(submission$day)
submission$age_bin <- cut(submission$age, breaks = c(-Inf, 20, 40, 60, Inf),
                             labels = c('<20', "20-40", "40-60", ">60"))
submission$age <- NULL
submission$job_binned <- car::recode(submission$job, "c('retired', 'student') = 'rs';
                                        c('blue-collar', 'entrepreneur')= 'be';
                                        c('housemaid', 'services')= 'hs'; 
                                        c('admin.', 'unknown')= 'au';
                                        c('management', 'self-employed','technician', 'unemployed') = 'others'")
submission$job_binned <- factor(submission$job_binned)
submission$job <- NULL
submission$balance_bin <- cut(submission$balance, 
                                 breaks = c(-Inf, 0, 500, 1000, 5000, Inf),
                                 labels = c("<=0", "1-500", "501-1000", "1001-5000", ">5000"))

submission$balance <- NULL
submission$day_binned <- car::recode(submission$day, "c(1,30) = 'special';
                        c(2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,
                                        22,23,24,25,26,27,28,29,31) = 'others'")
submission$day_binned <- factor(submission$day_binned)
submission$day <- NULL
submission_log <- submission
# Add constant in order to avoid non-positive values in log (monotonic transformation)
submission_log$duration <- submission_log$duration + 1
submission_log$pdays <- submission_log$pdays + 2
submission_log$previous <- submission_log$previous + 1

submission_log <- to_log(submission_log, log_varlist)

submission <- submission_log
```


Make the final prediction on the submission set
```{r}
ranger_finalGrid <- expand.grid(mtry=round(sqrt(length(df_train_preproc)) + 1),
                              splitrule= 'gini',
                              min.node.size=500)
set.seed(7)

ranger_final <- train(label~.,
                data = train_preproc,
                method = "ranger", num.trees=100,
                tuneGrid = ranger_finalGrid,
                metric = "recall",
                importance = "permutation",
                trControl = ctrl)

final_pred <- predict(ranger_final, newdata = submission)
```


