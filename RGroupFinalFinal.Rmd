---
title: "Bank Campaign Classification - Advanced R Group Project"
author: "Group 1- Louis Dubaere, Hatem Hassan, Federico Loguercio, Alberto Lombatti, Gerald Walravens, Anwita Bhure"
date: "6/6/2019"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, comment = NA, message = FALSE)

rm(list=ls())

load_reqs <- function(reqs) {
  for(pkg in reqs) {
    if (!(pkg %in% installed.packages())) { install.packages(pkg) }
    suppressPackageStartupMessages(library(pkg, character.only = T))
  }
}

# ADD LIBRARIES HERE
load_reqs(c("dplyr", "data.table", "ggplot2", "corrplot", "gridExtra", "DT",
            "grid", "doParallel", "caret", "MLmetrics", "DMwR", "car",
            "ranger", "xgboost", "deepboost", "highcharter","purrr", "tidyr","cowplot"))

```

```{r echo=FALSE, warning=FALSE}
# data import
train <- read.csv('data/BankCamp_train.csv')
submission <- read.csv('data/BankCamp_test.csv')
```

We are given a dataset that has information about whether a bank's customer responds positively to a marketing campaign and subscribes to a term deposit. The data has multiple variables of the customer, which can broadly be divided into three segments - Demographics, Transactional and Campaign-specific. 

### Data Exploration

We begin by undertaking basic quality and cosistency checks on the data.  

#### Check if there are any missing values in the training set and the submission set

```{r echo=TRUE, warning=FALSE}
any(is.na(train))
any(is.na(submission))
```

**As both return FALSE, there are no missing values**  

***

#### Check for the structure of the dataset
```{r echo=TRUE, warning=FALSE}
glimpse(train)
glimpse(submission)
```

```{r}
unique(train$y)
```

**The target variable is recorded as y, a factor with two levels either yes or no. It is therefore a binary classification problem.**

***

Before moving to modeling, we check the proportion of yes and no in the data.   

#### Check for imbalance
```{r echo=TRUE, warning=FALSE}
nrow(subset(train, y =='yes')) / nrow(train)
```

**Proportion of nos in the dataset is very low, ~11%. We may need to do resampling in order to get the right models.**   

***

#### Numerical Features
```{r echo=FALSE, fig.height=6, warning=FALSE}

numeric_vars <- unlist(lapply(train, is.numeric)) 
histograms = list()
for (i in colnames(train[numeric_vars])){
  histograms[[i]] <-
    ggplot(
      train[numeric_vars],
      aes_string(x=i,fill=train$y)) +
      geom_histogram(bins = 35) +
      theme_bw(base_size = 13)+
      labs(fill = "Target")
}
plot_grid(plotlist = histograms, ncol = 2)
```

**As can be seen from the histograms, most of the numerical features are heavily skewed, and need to be normalized**

***

#### Categorical Features
```{r echo=FALSE, fig.height=14, warning=FALSE}

cat_vars <- unlist(lapply(train, is.factor)) 
barPlots = list()
for (i in colnames(train[cat_vars])){
  if(i != 'y') {
      barPlots[[i]] <-
        ggplot(
          train[cat_vars],
          aes_string(x=i,fill=train$y)) +
          geom_bar() +
          theme_bw(base_size = 13)+
          labs(fill = "Target")
  }
}
plot_grid(plotlist = barPlots, ncol = 1)
```

***

#### Correlation between pairs of numerical features

```{r echo=TRUE, warning=FALSE}
train_num <- dplyr::select_if(train, is.numeric)
res <- cor(train_num)
corrplot.mixed(
          res,
          upper="circle",
          lower="number",
          tl.col = "black",
          number.cex = .8,
          tl.cex=.8)
```

**As shown from the result, there are no strong correlations between most of the pairs**

**Except the one for pdays and previous = 0.54**

***

#### Density Plots
```{r}
numeric_vars <- unlist(lapply(train, is.numeric)) 
histograms = list()
for (i in colnames(train[numeric_vars])){
  histograms[[i]] <-
    ggplot(
      train[numeric_vars],
      aes_string(x=i,fill=train$y)) +
      geom_density(alpha = 0.25) +
      theme_minimal() +
      labs(fill = "Target")
}
plot_grid(plotlist = histograms, ncol = 2)

```

**Apart from duration, none of the numerical variables clearly separates the target classes**

***

#### Preprocessing

```{r}
# encode target variable as 1 and 0 and convert it to factor
train$label <- as.factor(ifelse(train$y == 'yes', 1, 0))
train$y <- NULL

# split train data further into train and test (hold out)
test_proportion <- 0.2
set.seed(7)
train_index <-sample(nrow(train), floor(nrow(train)*(1-test_proportion)), replace = FALSE)

df_train <- train[train_index,]
df_test <- train[-train_index,]
```


## Baseline - Binary Logistic
```{r}
# Define custom summary function so we can get recall as output always

recallSummary <- function (data,
    lev = NULL,
    model = NULL) {
    c(Accuracy = MLmetrics::Accuracy(data$pred, data$obs),
      recall = MLmetrics::Recall(data$obs, data$pred, positive = 1))
}

ctrl_base <- trainControl(
  method = "cv", # k-fold cross val
  number = 5,
  savePredictions=TRUE,
  summaryFunction = recallSummary
)

bsline <- train(label~.,
             data = df_train,
             method = 'glm',
             family = "binomial",
             trControl = ctrl_base)

bsline
```

The model performs poorly in terms of recall, probably due to the unbalanced target variable. This can be tackled by either undersampling the prevalent class or oversampling the rare class or both. 

***

### Baseline with Resampling
The target variable is highly unbalanced. We will therefore resample it in order try to make the model better capture the underrepresented class.

Resampling is defined through the control object instead of outside, such that cross-validation performance will be evaluated on data that has not been resampled. Failing to do so would artifially inflate cv-recall, with a model that would perform far worse on an unbalanced test set.

```{r}
# define basic train control object 
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions=TRUE,
                     summaryFunction = recallSummary,
                     sampling = "down")
```

#### Undersampling
```{r}
set.seed(7)
log_under <- train(label~. ,
                   data = df_train,
                   method = 'glm',
                   family = "binomial",
                   trControl = ctrl)

log_under
```

This is a game changer. By simply undersampling, we are able to highly improve recall.

Let's try oversampling instead:

#### Oversampling
```{r}
ctrl$sampling <- "up"

set.seed(7)
log_up <- train(label~.,
                data = df_train,
                method = 'glm',
                family = "binomial",
                trControl = ctrl)

log_up
```

Next: "smote" for sampling

#### SMOTE
```{r}
ctrl$sampling <- "smote"

set.seed(7)
log_smt <- train(label~.,
                 data = df_train,
                 method = 'glm',
                 family = "binomial",
                 trControl = ctrl)

log_smt
```

Smote resampling takes accuracy up but brings down recall substantially. SMOTE stands for Synthetic Minority Oversampling Tecniques. It uses nearest neighbours algorithm to generate new and synthetic data. [1](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)

### Summary of all resampling results

Make bootstrapping resample-versions of each model in order to estimate the distribution of recall and accuracy for each.
```{r}
resample_models <- list(baseline = bsline,
                        undersampling = log_under,
                        oversampling = log_up,
                        SMOTE = log_smt
                        )

# do a resampling on the models and generate distribution of accuracy and recall metrics
resampling <- resamples(resample_models)
summ1 <- summary(resampling)
summ1

# or just print the train results
summ2 <- NULL
for (i in 1:length(resample_models)) {
  res <- resample_models[[i]]$results[1:3]
  summ2 <- rbind(summ2, res)
}
summ2$parameter <- names(resample_models)
datatable(summ2)
```

***

### Variable Transformations
Now that we have established that resampling improves model results significantly, we will keep it as the base and move to feature selection and feature engineering. We use undersampling method. 

#### Day as a categorical variable  
There is no numerical relevance to day of the month. Hence, it is converted as a factor. 

```{r}
train$day <- as.factor(train$day)
```


#### Age as a categorical variable
Proceed by grouping together age-ranges

```{r}
# create a new preprocessed dataset
train_preproc <- train

# bin age variable
train_preproc$age_bin <- cut(train_preproc$age, breaks = c(-Inf, 20, 40, 60, Inf),
                             labels = c('<20', "20-40", "40-60", ">60"))
train_preproc$age <- NULL
```

```{r}
df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
ctrl$sampling <- "down"

set.seed(7)
log_agebin <- train(label~.,
                    data = df_train_preproc,
                    method = 'glm',
                    family = "binomial",
                    trControl = ctrl)

log_agebin
```

```{r}
# update pipeline
summ2 <- rbind(summ2, log_agebin$results[1:3])
summ2$parameter[nrow(summ2)] <- "age binned"
datatable(summ2)
```

The results after binning age improve slightly. We now move on to the next variable, jobs. 

#### Reducing the number of categories in job

```{r}
levels(df_train_preproc$job)
```

Job currently has 12 different levels, but the proportion of observations in each of them is not uniform. We try merging some levels together to see if it has any impact on the model performance. We realise that reducing the levels will result in loss of granularity but it is assumed that strategies are not going to be planned at such minute levels and a general idea of customer's occupation would be enough.    
```{r}
train_preproc$job_binned <- car::recode(train_preproc$job, "c('retired', 'student') = 'rs';
                                        c('blue-collar', 'entrepreneur')= 'be';
                                        c('housemaid', 'services')= 'hs'; 
                                        c('admin.', 'unknown')= 'au';
                        c('management', 'self-employed','technician', 'unemployed') = 'others'")
train_preproc$job_binned <- factor(train_preproc$job_binned)

train_preproc$job <- NULL
```

```{r}
df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
set.seed(7)
log_agejobbin <- train(label~.,
                    data = df_train_preproc,
                    method = 'glm',
                    family = "binomial",
                    trControl = ctrl)

log_agejobbin
```

```{r}
# update pipeline
summ2 <- rbind(summ2, log_agejobbin$results[1:3])
summ2$parameter[nrow(summ2)] <- "age & job binned"
datatable(summ2)
```

The model recall decreases marginally. However, we can still go ahead with these two new features. 

#### Converting 'balance' to categorical 
This variable is highly skewed with negative values and majority 0. There are 3052 negative values and 2799 0s. We group the variable into ranges in order to cope with this.

```{r}
train_preproc$balance_bin <- cut(train_preproc$balance, 
                                 breaks = c(-Inf, 0, 500, 1000, 5000, Inf),
                                 labels = c("<=0", "1-500", "501-1000", "1001-5000", ">5000"))


train_preproc$balance <- NULL
```

```{r}
df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
set.seed(7)
log_bal <- train(label~.,
                 data = df_train_preproc,
                 method = 'glm',
                 family = "binomial",
                 trControl = ctrl)

log_bal
```

```{r}
# update pipeline
summ2 <- rbind(summ2, log_bal$results[1:3])
summ2$parameter[nrow(summ2)] <- "balance binned"
datatable(summ2)
```

Binning balance improves accuracy and recall.  

#### Modify Day Variable
The day variable is categorical not, but that means that it has 31 different levels. It can be imagined that certain days of the month have a slight effect on the target but most do not hold any explanatory power.

It is reasonable to argue that the first and the last day of the month are somewhat special. We group them together and put the rest into one category.

```{r}
train_preproc$day_binned <- car::recode(train_preproc$day, "c(1,30) = 'special';
                        c(2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,
                        22,23,24,25,26,27,28,29,31) = 'others'")
train_preproc$day_binned <- factor(train_preproc$day_binned)

train_preproc$day <- NULL

df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
log_day_binned <- train(label~.,
                 data = df_train_preproc,
                 method = 'glm',
                 family = "binomial",
                 trControl = ctrl)

log_day_binned
```

```{r}
summ2 <- rbind(summ2, log_day_binned$results[1:3])
summ2$parameter[nrow(summ2)] <- "Day binned"
datatable(summ2)
```

Model performance goes down but we are significantly reducing the number of variables which will shorten runtimes.

### pday binned
```{r}
train_preproc$pdays_binned <- cut(train_preproc$pdays, breaks = c(-Inf, 0, 99, 299, 499, Inf ),
                                  labels = c("notcontacted", "0-100", "101-300", "301-500", ">500"))
train_preproc$pdays <- NULL

df_train_preproc <- train_preproc[train_index,]
df_test_preproc <- train_preproc[-train_index,]
```

```{r}
log_pday_binned <- train(label~.,
                 data = df_train_preproc,
                 method = 'glm',
                 family = "binomial",
                 trControl = ctrl)

log_pday_binned
```

```{r}
summ2 <- rbind(summ2, log_pday_binned$results[1:3])
summ2$parameter[nrow(summ2)] <- "PDays binned"
datatable(summ2)
```


#### Redesigning how numeric variables are modelled

We now fit the model again, however, adding in Yeo-Johnson through preprocessing to fix skewness in all numeric variables. No other preprocessing is done. 

The Yeo-Johnson transformation is a variant of the BoxCox Transformation suited for variables with non-positive values. Variables are transformed to a more normal-like distribution with more stable variance.

```{r}
ctrl$sampling <- "down"

log_yj <- train(label~.,
                data = df_train,
                method = 'glm',
                family = "binomial",
                preProcess = "YeoJohnson",
                trControl = ctrl)

log_yj
```

The results remain mostly unchanged.
```{r}
# update pipeline
summ2 <- rbind(summ2, log_yj$results[1:3])
summ2$parameter[nrow(summ2)] <- "Yeo-Johnson"
datatable(summ2)
```

The Yeo-Johnson transformation does not appear to help with fixing skewness and outlier issues in certain variables.

#### Logarithm
Let's try taking the logarithm instead
```{r}
# Logarithmise skewed variables
to_log <- function(df, l_varlist){
  for(l_var in l_varlist){
    df[,l_var] <- log(df[,l_var])
  }
  return(df)
}

# Select the numeric variables that are skewed
log_varlist <- list('duration', 'campaign', 'previous')

train_log <- train_preproc

# Add constant in order to avoid non-positive values in log (monotonic transformation)
train_log$duration <- train_log$duration + 1

train_log$previous <- train_log$previous + 1

train_log <- to_log(train_log, log_varlist)

df_train_preproc <- train_log[train_index,]
df_test_preproc <- train_log[-train_index,]
```

```{r}
logarithm <- train(label~.,
                data = df_train_preproc,
                method = 'glm',
                family = "binomial",
                trControl = ctrl)

print(logarithm)

summ2 <- rbind(summ2, logarithm$results[1:3])
summ2$parameter[nrow(summ2)] <- "Logarithm"
datatable(summ2)
```

#### Parallelize
```{r}
cl <- makePSOCKcluster(4) # I have 4 cores, this can be adapted
registerDoParallel(cl)
# All subsequent models are run in parallel
```

***

## Optimization
Different models & hyperparameter tuning

### Random Forest
```{r}
set.seed(7)
# Several grids were explored, adapting the parameters in the direction where performance was improving.
# For faster runtimes, only a small grid is ran.
tuneGrid_ranger <- data.table(expand.grid(mtry=c(round(sqrt(length(df_train_preproc)) + 2),
                                                 round(1/2 * (length(df_train_preproc))),
                                                 round(2/3 * (length(df_train_preproc)))
                                                 ), #standard choices for mtry
                              splitrule= 'gini',
                              min.node.size=c(200, 300, 500))) # We found that very large values for min.node.size lead to the highest recall. Lower values optimize
# accuracy at the cost of recall.

ranger <- train(label~.,
                data = df_train_preproc,
                method = "ranger", num.trees=500, # increasing the number of trees, the model did not improve further
                tuneGrid = tuneGrid_ranger,
                metric = "recall",
                importance = "permutation",
                trControl = ctrl)

ranger

o1 <- data.frame(ranger$results[7,3:5])
names(o1) <- c("parameter", "Accuracy", "recall")
summ2 <- rbind(summ2, o1)
summ2$parameter[nrow(summ2)] <- "Random Forest"
datatable(summ2)
```

Calculate test set performance
```{r}
pred_ranger <- predict(ranger, newdata = df_test_preproc)
Accuracy(pred_ranger, df_test_preproc$label)
Recall(df_test_preproc$label, pred_ranger, positive = 1)
```


### Boosting trees

Deepboost is a crazy variant of boosting. It is optimized to be able to grow ensembles of very deep (very large) trees without overfitting like AdaBoost will when growing very deep trees. We grow very shallow trees with it and only select it because it allows to choose between logistic and exponential for the loss function. Logistic loss never assigns zero penalty to any points, making it more sensitive to outliers. Exponential loss penalizes incorrect corrections more strongly.

```{r}
# Again here, a far more exhaustive grid was explored iteratively.
tunegrid_deepbost <- expand.grid(num_iter = c(100), #more and larger trees improved accuracy at the cost of recall
                                 tree_depth = c(3),
                                 beta = c(0.1),
                                 lambda = 0.1,
                                 loss_type = c("e", "l"))

boosting <- train(label~.,
                data = df_train_preproc,
                method = "deepboost",
                tuneGrid = tunegrid_deepbost,
                trControl = ctrl,
                verbose = FALSE)

boosting

o2 <- data.frame(boosting$results[1,5:7])
names(o2) <- c("parameter", "Accuracy", "recall")
summ2 <- rbind(summ2, o2)
summ2$parameter[nrow(summ2)] <- "Boosting"
datatable(summ2)
```

Deepboost is not able to reach the levels of recall obtained with the random forest.


### XGBoost

```{r}
set.seed(7)

tunegrid_xgb <- expand.grid(nrounds = c(100, 80),
                       max_depth = c(5),
                       colsample_bytree = c(0.7),
                       eta = c(0.05),
                       gamma = c(0.1, 0.2),
                       min_child_weight = c(1),
                       subsample = 1
                      )

xgb <- train(label~.,
                data = df_train_preproc,
                method = "xgbTree",
                tuneGrid = tunegrid_xgb,
             metric = "recall",
                trControl = ctrl)

xgb

o3 <- data.frame(xgb$results[4,7:9])
names(o3) <- c("parameter", "Accuracy", "recall")
summ2 <- rbind(summ2, o3)
summ2$parameter[nrow(summ2)] <- "XGBoost"
datatable(summ2)
```

XGBoost performs quite well but still worse than random forest.

```{r}
mypred_xgb <- predict(xgb, newdata = df_test_preproc)
Accuracy(mypred_xgb, df_test_preproc$label)
Recall(df_test_preproc$label, mypred_xgb, positive = 1)
```


### Logistic Regression with regularization
Since we have seen that recall seems to be highest the less granularly the model fits the data, we will try whether a regularized logistic regression performs well.

```{r}
tuneGrid_reg = expand.grid(alpha = c(0, 0.5, 1), #alpha = 0 for Ridge and alpha = 1 for lasso
                                       lambda = c(0, 0.2, 0.5)) # higher lambda: more regularization

log_reg <- train(label~.,
                data = df_train_preproc,
                method = 'glmnet',
                family = "binomial",
                trControl = ctrl,
                tuneGrid = tuneGrid_reg)

print(log_reg)

o4 <- data.frame(log_reg$results[1,2:4])
names(o4) <- c("parameter", "Accuracy", "recall")
summ2 <- rbind(summ2, o4)
summ2$parameter[nrow(summ2)] <- "Logistic with Regularizations"
datatable(summ2)
```
Regularization significantly worsens performance.



```{r, include = F}
# These section are not included in the presentation

### XGBoost optimized for lift

# One last iteration was tried. Unfortunatly, caret does not allow for modification of the loss function. Ideally, we would want a cost function that highly penalizes false negatives, more so than false positives.
# 
# XGBoost allows for a custom evaluation function that goes a bit further than the custom evaluation function in caret. While we can not modify the loss function, the number of iterations can be more closely optimized for left hand side lift.



# Credit for this approach goes to:
# https://minimizeregret.com/post/2017/04/14/cost-sensitive-xgboost/
# whose code was closely replicated.

# We begin by building a lift-metric. We create a simulated profit which takes into account lost opportunity (simplified, of course there also is a cost for making the phone call, for false positives). Thereafter, lift being the improvement in profit compared to random calling is calculated. We use that metric for evaluation.

xgb_params <- list(objective = "binary:logistic",
                   eta = 0.05,
                   max_depth = 15,
                   colsample_bytree = 0.7,
                   subsample = 1,
                   min_child_weight = 1)

getProfit <- function(obs, pred, threshold = 0.5) {
  prob <- pred
  pred <- ifelse(pred >= threshold, 1, 0)
  tp_count <- sum(obs == 1 & obs == pred)
  tn_count <- sum(obs == 0 & obs == pred)
  fp_count <- sum(obs == 0 & obs != pred)
  fn_count <- sum(obs == 1 & obs != pred)
  profit <- tp_count * 3 - fn_count * 10
  
  return(profit)
}

getBenchmarkProfit <- function(obs) { # predict non-returning for everyone
  n <- length(obs)
  getProfit(obs, rep(0, times = n))
}

getLift <- function(probs, labels) {
  pred_profit <- as.numeric(getProfit(obs = labels,
                                      pred = probs,
                                      threshold = 0.5))
  naive_profit <- as.numeric(getBenchmarkProfit(labels))
  profit_lift <- pred_profit/naive_profit
  return(profit_lift)
}

xgb.getLift <- function(preds, data) {
    labels <- getinfo(data, "label")
    lift <- getLift(preds, labels)
    return(list(metric = "Lift", value = lift))
}

# Resamle outside since we are not using caret
df_train_resampled <- upSample(x = df_train_preproc[, -12],
                     y = df_train_preproc$label)    
df_test_notresampled <- df_test_preproc
df_test_notresampled$Class <- df_test_notresampled$label
df_test_notresampled$label <- NULL

# Need to create matrix object for ranger
datamatrix <- model.matrix(Class~., data = df_train_resampled)[, -1]
y <- as.numeric(df_train_resampled$Class) - 1
testdatamatrix <- model.matrix(Class~., data = df_test_notresampled)[, -1]
ytest <- as.numeric(df_test_notresampled$Class) - 1

xgb_fit <- xgb.cv(params = xgb_params, data = datamatrix, label = y, nfold = 5,
                  feval = xgb.getLift, maximize = TRUE, 
                  nrounds = 500, early_stopping_rounds = 50,
                  verbose = TRUE, print_every_n = 100)
```


```{r, include = F}
xgb_pred<- xgb.train(params = xgb_params, data = xgb.DMatrix(data = datamatrix, label = y),
                  feval = xgb.getLift, maximize = TRUE, 
                  nrounds = 500, early_stopping_rounds = 50,
                  watchlist = list(val = xgb.DMatrix(data = testdatamatrix, label = ytest)),
                  verbose = TRUE, print_every_n = 100)
```

```{r, include = F}
temp_prob <- predict(xgb_pred, newdata = xgb.DMatrix(data = testdatamatrix, label = ytest), 
                      type = 'response')
temp_preds <- ifelse(temp_prob >= 0.5, 1, 0)
Accuracy(temp_preds, ytest)
Recall(ytest, temp_preds, positive = 1)

# We do not further optimize this poor result, the last variable transformation significantly worsened performance and it never performed as well as the random forest.
```

***

## Final Prediction

```{r echo=F, include=F}
# Prepare submission dataset

submission$day <- as.factor(submission$day)
submission$age_bin <- cut(submission$age, breaks = c(-Inf, 20, 40, 60, Inf),
                             labels = c('<20', "20-40", "40-60", ">60"))
submission$age <- NULL
submission$job_binned <- car::recode(submission$job, "c('retired', 'student') = 'rs';
                                        c('blue-collar', 'entrepreneur')= 'be';
                                        c('housemaid', 'services')= 'hs'; 
                                        c('admin.', 'unknown')= 'au';
                                        c('management', 'self-employed','technician', 'unemployed') = 'others'")
submission$job_binned <- factor(submission$job_binned)
submission$job <- NULL
submission$balance_bin <- cut(submission$balance, 
                                 breaks = c(-Inf, 0, 500, 1000, 5000, Inf),
                                 labels = c("<=0", "1-500", "501-1000", "1001-5000", ">5000"))

submission$balance <- NULL
submission$day_binned <- car::recode(submission$day, "c(1,30) = 'special';
                        c(2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,
                                        22,23,24,25,26,27,28,29,31) = 'others'")
submission$day_binned <- factor(submission$day_binned)
submission$pdays_binned <- cut(submission$pdays, breaks = c(-Inf, 0, 99, 299, 499, Inf ),
                                  labels = c("notcontacted", "0-100", "101-300", "301-500", ">500"))
submission$pdays <- NULL

submission$day <- NULL
submission_log <- submission
# Add constant in order to avoid non-positive values in log (monotonic transformation)
submission_log$duration <- submission_log$duration + 1
#submission_log$pdays <- submission_log$pdays + 2
submission_log$previous <- submission_log$previous + 1

submission_log <- to_log(submission_log, log_varlist)

submission <- submission_log
```


Make the final prediction on the submission set
```{r}
ranger_finalGrid <- expand.grid(mtry=round(1/2 * (length(df_train_preproc))),
                              splitrule= 'gini',
                              min.node.size=200)
set.seed(7)

ranger_final <- train(label~.,
                data = train_log,
                method = "ranger", num.trees=500,
                tuneGrid = ranger_finalGrid,
                metric = "recall",
                importance = "permutation",
                trControl = ctrl)

final_pred <- predict(ranger_final, newdata = submission)
```

```{r}
#write.csv(final_pred, "/Users/federicologuercio/Desktop/final_preds.csv")
```
